{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E2TCC3iM39Pc"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd \"drive/MyDrive/nlp\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_Zawfv92oAE"},"outputs":[],"source":["!pip install -q -U trl transformers accelerate peft datasets bitsandbytes evaluate git+https://github.com/huggingface/huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xmr3WzQCy4ES","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11072,"status":"ok","timestamp":1714747058067,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"hbC9pZlGy4EX","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","model_name = \"google-bert/bert-base-uncased\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GfewFrCNy4EZ","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"lenatr99/Slovene_SuperGLUE_CB\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eU5WzY6Gy4Ea","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/orange4/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"p_jEbI9zy4Eb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["id2label = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n","label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(id2label),\n","    id2label=id2label,\n","    label2id=label2id,\n","    trust_remote_code=True\n",")\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":429,"status":"ok","timestamp":1714747122732,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"PVdC20uCy4Ec","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["CONTEXT_COL = \"premise\"\n","HYPOTHESIS_COL = \"hypothesis\"\n","\n","def preprocess_function(examples):\n","    \"\"\"\n","    The preprocessing function prepares examples for processing by the model.\n","    It concatenates premise and hypothesis for each example to form a single input string.\n","    \"\"\"\n","    inputs = [f\"{premise} {hypothesis}\" for premise, hypothesis in zip(examples[CONTEXT_COL], examples[HYPOTHESIS_COL])]\n","    tokenized_examples = tokenizer(inputs, truncation=True)\n","    if \"label\" in examples:\n","        tokenized_examples[\"labels\"] = [label2id[label] for label in examples[\"label\"]]\n","    return tokenized_examples"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9SW79a10y4Ed","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["tokenized_dataset = dataset.map(preprocess_function,\n","                                remove_columns=['idx', 'premise', 'hypothesis', 'label'], batched=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sJIffaKGy4Ee"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","        num_rows: 110\n","    })\n","    eval: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","        num_rows: 22\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12861,"status":"ok","timestamp":1714747154389,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"bBU_njk_y4Ee","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1714747167587,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"oqshUt8Ty4Ef","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1}"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1714747177552,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"dULP1dwXy4Ef","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from transformers import set_seed\n","\n","set_seed(42)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"WUnJCl-xy4Eg","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n","\n","NUM_VIRTUAL_TOKENS = 12\n","\n","peft_config = PromptTuningConfig(\n","    peft_type=\"PROMPT_TUNING\",\n","    task_type=TaskType.SEQ_CLS,\n","    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n","    num_layers=6,\n","    token_dim=768,\n","    num_attention_heads=12,\n","    tokenizer_name_or_path=model_name #The pre-trained model\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"O5TnAXVry4Eg","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 11,523 || all params: 109,496,070 || trainable%: 0.0105\n"]}],"source":["model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":769,"status":"ok","timestamp":1714748503603,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"1t78XAkky4Eh","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","new_model_name = \"prompt_fine_tuned_CB_bert\"\n","\n","training_args = TrainingArguments(\n","    output_dir=new_model_name,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    evaluation_strategy='steps',\n","    max_steps=400,\n","    use_cpu=False,\n","    load_best_model_at_end=True\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Rd6tJoK5y4Eh","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset=tokenized_dataset['eval'],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LUiW-slqy4Ei","jupyter":{"outputs_hidden":false}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d9d22da66a54137b52ec26cac74991c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 80.1766, 'train_samples_per_second': 4.989, 'train_steps_per_second': 4.989, 'train_loss': 0.8893228149414063, 'epoch': 3.64}\n"]},{"data":{"text/plain":["TrainOutput(global_step=400, training_loss=0.8893228149414063, metrics={'train_runtime': 80.1766, 'train_samples_per_second': 4.989, 'train_steps_per_second': 4.989, 'total_flos': 23978397724524.0, 'train_loss': 0.8893228149414063, 'epoch': 3.6363636363636362})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"elapsed":14444,"status":"ok","timestamp":1714749042880,"user":{"displayName":"Tjaša Domadenik","userId":"12065373928021713454"},"user_tz":-120},"id":"VSBAfKZ7y4Ei","outputId":"c885bea2-13cf-4984-b742-408dc3cb8d7b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe35d21e281c4e708ddb92c7fc66f4eb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 1.1635671854019165,\n"," 'eval_accuracy': 0.3181818181818182,\n"," 'eval_f1': 0.1536050156739812,\n"," 'eval_runtime': 6.2591,\n"," 'eval_samples_per_second': 3.515,\n"," 'eval_steps_per_second': 3.515,\n"," 'epoch': 3.6363636363636362}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2VRgBfr3y4Ej","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/orange4/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc9e310fb6c643a781980be9a531ea1c","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77ee1c2a06724def982ff6e073bf71b8","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c161a66747a645e289d47102036eb50d","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/lenatr99/prompt_fine_tuned_CB_bert/commit/85fa9007e58c11eb04dc7bcf496d67adf781110b', commit_message='prompt_fine_tuned_CB_bert', commit_description='', oid='85fa9007e58c11eb04dc7bcf496d67adf781110b', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub(new_model_name)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"UHk5mXb9y4Ej","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# Example\n","hypothesis =\"Valence je pomagal\"\n","premise = \"Valence praznoglavi, Valence krepostni kreten. Zakaj si ga tip raje ni zataknil v ustrezen del lastne titanske anatomije? Je morda mislil, da mi pomaga?\""]},{"cell_type":"code","execution_count":19,"metadata":{"id":"-TFTYSmIy4Ek"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cbf8b2076c345bc969e69bda13371ee","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6021ef9fd8ee4f3d81c79f8afb8af71e","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ac681c0c43f460fa26018dcec89eaf8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d4bba8c288c44fb80ff976a1d7a10dc","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# We need to set the seed, otherwise some weights of the model are initialized differently every time, and consequently the result can be different each time as well\n","# set_seed(42)\n","import torch\n","\n","adapter_name = \"lenatr99/\" + new_model_name\n","\n","tokenizer = AutoTokenizer.from_pretrained(adapter_name)\n","inputs = tokenizer(f\"{premise} {hypothesis}\", return_tensors=\"pt\")\n","label = torch.tensor([1]).unsqueeze(0)  # Batch size 1"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"ie-qeEHAy4Ek"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(id2label),\n","    id2label=id2label,\n","    label2id=label2id,\n","    trust_remote_code=True\n",")\n","outputs = model(**inputs, labels=label)\n","logits = outputs.logits"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"pJPaVBZvy4Ek","jupyter":{"outputs_hidden":false}},"outputs":[{"data":{"text/plain":["2"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Print prediction\n","logits.argmax().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geaH5qify4El"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
